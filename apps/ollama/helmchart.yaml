# clusters/<your-cluster-name>/ollama-helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ollama
spec:
  interval: 10m
  chart:
    spec:
      chart: ollama
      version: "1.24.0"
      sourceRef:
        kind: HelmRepository
        name: otwld-ollama-helm
        namespace: flux-system
  targetNamespace: ollama
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    # Pod scheduling constraints
    nodeSelector:
      kubernetes.io/hostname: talos-cp-01
    tolerations:
      - key: "storage"
        operator: "Equal"
        value: "local"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"

    # Ollama specific configurations
    ollama:
      gpu:
        enabled: true
        type: "nvidia"
        number: 1
      models:
        pull:
          - llama2
          - mistral
          # Add more models here as needed

      # --- START: Probe Configuration ---
      probes:
        startupProbe:
          enabled: true
          # Start checking after 10 seconds.
          # Give it a very generous threshold and period for initial model downloads.
          # 180 * 10s = 1800 seconds = 30 minutes. Adjust based on model size & network.
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 180 # Allows for 30 minutes of startup time
          timeoutSeconds: 5
        readinessProbe:
          enabled: true
          # Give it some initial delay even after startup probe for safety
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3
          timeoutSeconds: 5
        livenessProbe:
          enabled: true
          # Start checking liveness after it's been ready for a bit
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3
          timeoutSeconds: 5
      # --- END: Probe Configuration ---

      # Service configuration (default is ClusterIP)
      # ... rest of your service and resource config ...
    service:
      type: ClusterIP
      port: 11434
      name: ollama

    resources:
      requests:
        cpu: 8000m
        memory: 16Gi
        nvidia.com/gpu: 1
      limits:
        cpu: 12000m
        memory: 16Gi
        nvidia.com/gpu: 1

    # Persistence configuration
    persistentVolume:
      enabled: true
      existingClaim: ollama-models-pvc
