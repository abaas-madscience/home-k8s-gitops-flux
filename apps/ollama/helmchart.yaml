# clusters/<your-cluster-name>/ollama-helmrelease.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ollama # Deploy into the ollama namespace
spec:
  interval: 10m
  chart:
    spec:
      chart: ollama
      version: "0.9.6"
      sourceRef:
        kind: HelmRepository
        name: otwld-ollama-helm
        namespace: flux-system
  targetNamespace: ollama
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    # Pod scheduling constraints
    nodeSelector:
      kubernetes.io/hostname: talos-cp-01
    tolerations:
      - key: "storage"
        operator: "Equal"
        value: "local"
        effect: "NoSchedule"

    # Ollama specific configurations
    ollama:
      gpu:
        enabled: true # Enable GPU integration
        type: "nvidia" # Specify GPU type
        number: 1 # Request 1 GPU
      models:
        pull:
          - llama2 # Example model to download on startup
          - mistral # Another example model
        # You can add more models here, e.g.:
        # - "llama3:8b"
        # - "codellama"
      # Configure Ollama API host (optional, usually default is fine)
      # extraEnv:
      #   - name: OLLAMA_HOST
      #     value: "0.0.0.0:11434"

    # Persistence configuration
    persistentVolume:
      enabled: true # Enable persistence for models
      existingClaim: ollama-models-pvc # Use the PVC we pre-created
      # size: 50Gi # This size is defined in the PVC, but can be overridden here if chart allows
      # storageClass: "openebs-hostpath" # Not needed if using existingClaim, but good to know

    # Service configuration (default is ClusterIP)
    service:
      type: ClusterIP
      port: 11434
      name: ollama # The service name will be 'ollama' in 'ollama' namespace

    # Resource requests and limits for the Ollama container
    resources:
      requests:
        cpu: 8000m # 8 CPU cores
        memory: 16Gi # 16GB RAM (adjust based on models and usage)
        nvidia.com/gpu: 1 # Request 1 GPU
      limits:
        cpu: 12000m # 12 CPU cores
        memory: 16Gi # 24GB RAM (adjust based on models and usage)
        nvidia.com/gpu: 1 # Limit to 1 GPU
